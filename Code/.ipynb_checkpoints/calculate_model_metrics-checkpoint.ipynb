{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions\n",
    "1. lift_plot_model: Function to plot Lift Chart\n",
    "2. plot_roc: Function to plot ROC Chart\n",
    "3. evaluate_model: Function to calculate and return key model performance metrics and return them in a dataframe\n",
    "4. plot_grid_search: Function to plot the validation curve for the GridSearchCV paramter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lift_plot_model(ytest, yprob):\n",
    "    '''\n",
    "    Objective: Function to plot Lift Chart\n",
    "    Argument : Actual Take up rate(1/0), predicted probabilities\n",
    "    Returns  : Lift chart, Lift table\n",
    "    Output   : Lift Chart\n",
    "\n",
    "    '''\n",
    "\n",
    "    n_bins = 10\n",
    "\n",
    "    actual_ser = pd.Series(ytest).rename('actuals').reset_index()\n",
    "    proba_ser = pd.Series(yprob).rename('probabilities').reset_index()\n",
    "\n",
    "    # Join table and drop indicies\n",
    "    lift_table = pd.concat([actual_ser, proba_ser], axis=1).fillna(0)\n",
    "    #lift_table.drop('index', inplace=True)\n",
    "    actual_col = 'actuals'\n",
    "\n",
    "    probability_col = 'probabilities'\n",
    "    \n",
    "    lift_table.sort_values(by=probability_col, ascending=False, inplace=True)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Split the data into the number of bins desired.\n",
    "    for group in np.array_split(lift_table, n_bins):\n",
    "        score = group[(group[actual_col] == 1)][actual_col].sum()\n",
    "\n",
    "        rows.append({'NumCases': len(group), 'NumCorrectPredictions': score})\n",
    "\n",
    "    lift = pd.DataFrame(rows)\n",
    "\n",
    "    #Cumulative Gains Calculation\n",
    "    lift['RunningCompleted'] = lift['NumCases'].cumsum() - lift['NumCases']\n",
    "\n",
    "    lift['PercentCorrect'] = lift['NumCorrectPredictions'].cumsum() / \\\n",
    "    lift['NumCorrectPredictions'].sum() * 100\n",
    "\n",
    "    lift['AvgCase'] = lift['NumCorrectPredictions'].sum() / len(lift)\n",
    "    lift['CumulativeAvgCase'] = lift['AvgCase'].cumsum()\n",
    "    #lift['PercentAvgCase'] = lift['CumulativeAvgCase'].apply(\n",
    "    #    lambda x: (x*1.0 / lift['NumCorrectPredictions'].sum()) * 100)\n",
    "\n",
    "    #Lift Chart\n",
    "    lift['LiftLine'] = 1\n",
    "    lift['Lift'] = lift['NumCorrectPredictions'] / lift['AvgCase']\n",
    "\n",
    "    plt.plot(lift['Lift'], label= 'Response rate for model');\n",
    "\n",
    "    plt.plot(lift['LiftLine'], 'r-', label='Normalised \\'response rate\\' \\\n",
    "    with no model');\n",
    "\n",
    "    plt.xlabel(str(100/len(lift)) + '% Increments');\n",
    "    plt.ylabel('Lift');\n",
    "    plt.legend();\n",
    "    plt.title(\"Lift Chart\");\n",
    "    plt.show();\n",
    "    return lift\n",
    "    #plt.gcf().clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(ytest_roc,yprob_roc):\n",
    "        '''\n",
    "        Objective: Function to plot ROC Graph\n",
    "        Argument : ytest: Actual Take up rate(1/0), yprob: predcicted probabilities\n",
    "        Returns  : ROC Plot\n",
    "        Output   : ROC Plot\n",
    "\n",
    "        '''\n",
    "        fig = plt.figure(1, figsize=(6, 6));\n",
    "\n",
    "        false_positive_rate, true_positive_rate, thresholds = \\\n",
    "        roc_curve(ytest_roc, yprob_roc)\n",
    "\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "        plt.title(\"Receiving Operator Characteristic\");\n",
    "\n",
    "        plt.plot(false_positive_rate, true_positive_rate, 'b', \\\n",
    "        label='AUC = %0.2f' % roc_auc);\n",
    "\n",
    "        plt.legend(loc='lower right');\n",
    "        plt.plot([0,1], [0,1], 'r--');\n",
    "        plt.xlim([-0.1, 1.2]);\n",
    "        plt.ylim([-0.1, 1.2]);\n",
    "        plt.ylabel(\"True Positive Rate\");\n",
    "        plt.xlabel(\"False Positive Rate\");\n",
    "        plt.tight_layout();\n",
    "\n",
    "        nfig = plt.figure(2, figsize=(6, 6));\n",
    "        plt.show();\n",
    "        #plt.gcf().clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, trained_model, xtrain, xtest, ytrain, ytest, verbose = False, threshold = 0.5):\n",
    "    '''\n",
    "        Objective: Function to calculate and return key model performance metrics\n",
    "        Arguments: 7 arguments\n",
    "                    1) model_name: Name of the model\n",
    "                    2) trained_model: Trained model\n",
    "                    3) xtrain: Training data set for features\n",
    "                    4) xtest: testing dataset for features\n",
    "                    5) ytrain: Training data set for target\n",
    "                    6) ytest: testing dataset for target\n",
    "                    7) verbose: print key performance metrics if True (default False)\n",
    "                    8) threshold: Decision threshold used to classify the predicted probabilities\n",
    "        Returns  : pd.DataFrame containing all key performance metrics\n",
    "        Output   : pd.DataFrame containing all key performance metrics, ROC plot, Lift plot\n",
    "\n",
    "    '''\n",
    "    #Predict using trained model for training and test datasets (with and without probabilities) \n",
    "    prob_test = trained_model.predict_proba(xtest)\n",
    "    prob_train = trained_model.predict_proba(xtrain)\n",
    "    pred_test = (prob_test [:,1] >= threshold).astype('int')\n",
    "    pred_train = (prob_train [:,1] >= threshold).astype('int')\n",
    "    \n",
    "    #Calculate AUC\n",
    "    auc_score = roc_auc_score(ytest, prob_test[:,1])\n",
    "    \n",
    "    #Calculate train and test accuracy\n",
    "    train_acc = accuracy_score(ytrain.values.ravel(), pred_train)\n",
    "    test_acc = accuracy_score(ytest.values.ravel(), pred_test)\n",
    "   \n",
    "    #Calculate log loss value\n",
    "    log_loss_value = log_loss(ytest, prob_test[:,1],eps=1e-15, normalize=True)\n",
    "    \n",
    "    #Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(ytest.values.ravel(), pred_test)\n",
    "\n",
    "    #Calculate classification model evaluation metrics like precision, recall, f1 score\n",
    "    report = classification_report(ytest, pred_test)\n",
    "    precision,recall,fscore,support=precision_recall_fscore_support(ytest,pred_test)\n",
    "    \n",
    "    \n",
    "    print (\"Lift plot for validation Sample\")\n",
    "    lift_table = lift_plot_model(ytest.values.ravel(), prob_test[:,1])\n",
    "\n",
    "    print (\"ROC curve for the validaton Sample\")\n",
    "    plot_roc(ytest.values.ravel(), prob_test[:,1])\n",
    "    \n",
    "    #Collate all key performance metrics into a dataframe\n",
    "    model_evaluation_metrics = pd.DataFrame({'Model': [model_name], 'AUC': [auc_score], 'Test Accuracy': [test_acc]\n",
    "              , 'Recall_1': [recall[1]], 'Precision_1': [precision[1]], 'F1 Score_1': [fscore[1]]                           \n",
    "              , 'Log loss': [log_loss_value]})\n",
    "    model_evaluation_metrics.columns\n",
    "    model_evaluation_metrics = model_evaluation_metrics[['Model', 'AUC', 'Test Accuracy',\n",
    "                                                    'Recall_1', 'Precision_1', 'F1 Score_1','Log loss']]\n",
    "\n",
    "    #Get lifts in top n deciles, where n is defined below\n",
    "    n=2\n",
    "    lift_table.reset_index(inplace = True)\n",
    "    lift_table['index'] = lift_table['index'] + 1\n",
    "    lift_table['Decile'] = lift_table['index'].apply(lambda x: 'Decile_' + str(x) + ' Lift %')\n",
    "    top_decile_lifts = lift_table[0:n][['Decile','Lift']].T\n",
    "    top_decile_lifts.columns = top_decile_lifts.iloc[0]\n",
    "    top_decile_lifts = top_decile_lifts.reindex(top_decile_lifts.index.drop('Decile'))\n",
    "    top_decile_lifts.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    #Add lift values for top deciles to the model key performance metrics\n",
    "    model_evaluation_metrics = pd.concat([model_evaluation_metrics,top_decile_lifts],axis=1)\n",
    "    \n",
    "    #Print key performance metrics if verbose = True is passed as an argument\n",
    "    if verbose == True:\n",
    "        print (\"Trained model :: \", trained_model)\n",
    "        print (\"\\n\\nModel ROC-AUC score for validation sample: %.3f\" \\\n",
    "                                          % auc_score)\n",
    "        print (\"\\n\\nTrain Accuracy :: \", train_acc)\n",
    "        print (\"\\n\\nTest Accuracy :: \", test_acc)\n",
    "        print (\"\\n\\nLog Loss Without for validation sample:\", \\\n",
    "                        log_loss_value)\n",
    "        \n",
    "        print (\"\\n\\n Confusion matrix \\n\")\n",
    "        skplt.metrics.plot_confusion_matrix(ytest.values.ravel(), pred_test, title=\"Confusion Matrix\",\n",
    "            figsize=(4,4),text_fontsize='large')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\\n Classification report (weighted average across classes) ::\\n\", classification_report(ytest, pred_test))\n",
    "\n",
    "    return model_evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    '''\n",
    "        Objective: To plot Validation Curve for GridSearchCV parameter tuning results \n",
    "        Arguments: 5 arguments\n",
    "                    1) cv_results: Cross validation results from tuning\n",
    "                    2) grid_param_1: List of parameter 1 values used for tuning\n",
    "                    3) grid_param_2: List of parameter 2 values used for tuning\n",
    "                    4) name_param_1: Parameter 1 name\n",
    "                    5) name_param_2: Parameter 2 name\n",
    "        Output   : Validation Curve plot with both parameters and CV results\n",
    "\n",
    "    '''\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"lower right\", fontsize=8)\n",
    "    ax.grid('on')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
